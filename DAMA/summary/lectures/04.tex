\section{Similarity Search}

In the task of determining the similarity between users $U_1$ and
$U_2$ based on the sets of movies they have watched, a naive approach
involves comparing all possible user pairs. This results in a
computational complexity of $O(n^2)$, where $n$ is the number of
users. To address this challenge efficiently, we focus on identifying
pairs of users with high similarity.

\subsection{Jaccard Similarity}

One commonly used metric for measuring the similarity between two
sets is the Jaccard Similarity. This metric quantifies the overlap
between two sets by comparing the size of their intersection to the
size of their union.

\begin{definition}[Jaccard Similarity]
  \begin{displaymath}
    \text{Similarity}(C_1, C_2) = \frac{
      \left|C_1 \cap C_2\right|
    }{
      \left|C_1 \cup C_2\right|
    }
  \end{displaymath}
\end{definition}

The assumption here is that we don't have to bother about all the movies
that we didn't watch together, it only takes into account the movies that
we did see.

\subsection{Candidate Selection for Similar Pairs}

In this case we would still have to execute this similarity calculation on
all possible user pairs, even those that have a very small overlap. To select
a set of users that we will run similarity tests on, we can use min-hashing
to generate signatures and use locality-sensitive hashing to generate candidate
pairs to test for similarity.

First, we run a min-hashing algorithm to generate signatures, which are short
integer vectors that represent the sets and reflects their similarity. Then, we
run locality-sensitive hashing to get pairs of signatures that we need to test
for similarity.

\subsubsection{Shingles}

If we don't look at user-movie selection, but rather document-text selection to
do for example plagiarism checks, we can use shingles (or grams) as a measure of
similarity. For example, if we have a document $abcab$ and we take $2-grams$ we
get the set of ${ab, bc, ca}$. Alternatively, instead of characters, we can use
a sequence of $k$ consecutive words. For example, $child in his eyes$
will result
in ${child in, in his, his eyes}$.
Finally, we represent the document by a binary column of the set of
$k$-shingles.
Documents with a lot of shingles in common have similar text, even if the text
appears in a different order. However, you need to pick $k$ large
enough, or most
document will have most shingles.
To compress long shingles, we can hash them to uints as we can easily
compare and
store such values.

\subsubsection{Minhashing}

We can take the column of the user-movie, or document-shingles binary matrix
and replace the binary columns with a short signature. The goal is that if the
columns are similar, the signatures should be similar. We can't sample from the
rows and let the signature be those bits only, as the input matrix is likely to
be very sparse. Rather, we use the notion that the ordering of the matrix is not
relevant to the similarity calculation.

\begin{definition}[Minhashing]
  ~
  \begin{itemize}
    \item Permute (swap) the rows randomly (or generate a random
      sequence of row indices)
    \item Define hash function $h(C)$ as the position of the first
      permuted row in $C$ that has a $1$.
    \item Run this several (i.e. 100) times independently and
      concatenate to create a signature
    \item The similarity of two signatures is then the fraction of
      signature elements that match
  \end{itemize}
\end{definition}

We see that the probability that $h(C_1) = h(C_2)$ is very close to
$\text{Sim}(C_1, C_2)$. Consider that if we check two hashes and they
are not the same, we note that down as a type $S$ row. If they are equal
we note that down as a type $B$ row. The overall probability is then
$\frac{B}{B + S}$ or, the equal over the total rows, which is the same
as the Jaccard Similarity (intersection over union).

The main challenge of minhashing arises when dealing with a very
large number of entries, as generating and storing a true random
permutation of rows becomes computationally expensive and
memory-intensive. Furthermore, accessing rows in the permuted order
can lead to inefficient memory access patterns, such as thrashing or
excessive swapping. To address this, instead of explicitly permuting
rows, we use $n \rightarrow n$ hash functions to simulate the effect
of permutations. By processing the matrix top-down row by row, we
update the signature values for each column and each hash function in
parallel, keeping track of the minimum hash values. This approach
avoids the need for explicit permutations.

\begin{definition}[Minhashing without Permutation]
  ~
  \begin{itemize}
    \item Define a family of $n \rightarrow n$ hash functions $h_1,
      h_2, \dots, h_k$ to simulate the effect of row permutations.
    \item Initialize the signature matrix $\text{Signature}[i][C]$
      with $\infty$ for all hash functions \(i\) and columns \(C\).
    \item Process the binary matrix row by row:
      \begin{itemize}
        \item For each row \(r\):
          \begin{itemize}
            \item Compute \(h_i(r)\) for all hash functions \(h_1,
              h_2, \dots, h_k\).
            \item For each column \(C\) in that row where the matrix
              has a \(1\):
              \begin{itemize}
                \item For each hash function \(h_i\), update the
                  signature value:
                  \begin{align*}
                    \text{Signature}[i][C] =
                    \min(\text{Signature}[i][C], h_i(r))
                  \end{align*}
              \end{itemize}
          \end{itemize}
      \end{itemize}
    \item The resulting signature matrix stores the smallest hash
      values observed for each column and each hash function,
      effectively simulating the effect of random row permutations.
  \end{itemize}
\end{definition}

\subsubsection{Locality-Sensitive Hashing}

Now that we have converted the large and sparse binary matrix to a
smaller matrix of similarity hashes, the memory constraint is
removed. However, calculating the real similarities between the
pairs is still an expensive task. The idea behind locality-sensitive
hashing is to split the columns of the signature matrix into bands of
the same size. If two columns are very similar, then it is likely that
at least one band will be identical.

Instead of first calculating the bands, we can determine candidate
pairs from columns that hash at least once to the same bucket. Split
the rows up into bands and calculate a number of hashes for all columns
in the bands. When two columns hit the same bucket, consider them similar
and record the pair.

\begin{definition}[Locality-Sensitive Hashing]
  ~
  \begin{itemize}
    \item Split the signature matrix in $b$ bands
    \item For over all the columns $c$ in all bands calculate
      hashes that map to $k$ buckets.
    \item Whenever a column $c$ in a band maps to the same bucket
      as another column, consider them similar and record the pair.
  \end{itemize}
\end{definition}

Summarizing, the probability that the signatures agree on one row is
$s$ (the Jaccard similarity) and $P_\text{band} = s^k$ that they agree
on one particular band (or $1 - s^k$ that they are not identical).
The probability that they are not identical in any of the $b$ bands is
$(1 - P_\text{band})^b$. Finally, the probability of becoming a candidate
pair is $t \approx \frac{1}{b}^{\frac{1}{r}} = \sqrt[r]{\frac{1}{b}}$
