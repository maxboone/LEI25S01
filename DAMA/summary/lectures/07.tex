\section{Random Forests and Ensembles}

Random Forests are supervised learning models that can be used for
both classification and regression tasks. They are based on an
ensemble of decision trees, which individually split the dataset into
subsets by evaluating feature values. These splits occur at multiple
levels, enabling the model to progressively refine its predictions
for the target variable.

\subsection{Ensembles}

An ensemble in supervised learning is a collection of models that
works by aggregating the individual predictions. Generally, it is more
accurate than the base model. Regression generally averages the indivdual
predictions, and classification uses a majority vote.

It helps if there is more diversity between models, which can be achieved by
using randomization or multiple types of classifier models.

\subsubsection{Bagging}

Bootstrap Aggregating, in short Bagging, is an early implementation of this
idea. Here each tree is bootstrapped with random samples with replacement
from the original dataset.

\begin{definition}[Bagging]
  \begin{itemize}
    \item Take random samples with replacement
    \item Given a training set $D$ of size $n$, generate $m$
      new training sets $D_1,...,D_m$ each of size $n$.
    \item Replacement means that some observations will be
      repeated in each sample.
    \item For a large $n$, $D_i$ will have a fraction of $(1 -
      \frac{1}{e})$ samples.
      Meaning, around $63.2\%$ unique samples and $36/8\%$ duplicates.
  \end{itemize}
\end{definition}

Overfitting is avoided in this case as the learnes have little
correlation as they learn from different datasets. The optimal
amount of learners can be determined by cross-validation or OOB 
estimation.

\subsubsection{Random Subspace Method}

A follow-up of bagging is the random subspace method. Instead of
sampling, we build each tree in the ensemble from a random subset of
the attributes. This mainly works for high-dimensional problems and
individual trees won't over-focus on attributes that appear the most
predictive in the training set.

\subsubsection{Random Forests}

Finally, random forests first select a set of random attributes and
set the best attributes from the random subset on each node. The amount
of attributes is $\sqrt{p}$ for classification and $\frac{p}{3}$ attributes
for regression. It also uses out-of-bag estimation:

  \begin{definition}[Out of Bag Estimation]
    
  \end{definition}
