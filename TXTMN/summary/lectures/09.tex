\section{Text Mining in Practice}

There are two forms of summarization, extractive summarization that is
composed completely of material from the source and abstractive summarization
that contains material not originally in the sources, but shorter paraphrases.

\begin{definition}[Extractive summarization]
  Completely composed of material from the source.
\end{definition}

\begin{definition}[Abstractive Summarization]
  Contains material not originally in the source, but shorter paraphrases
\end{definition}

An example of extractive summarization is a search engine snippet,
that literally
comes from the document but is not altered. Abstractive summarization
generates a
completely new summarization from the source material.

Extractive summarization is generally a supervised task, where we
classify each sentence
for inclusion or exclusion from the summary. Alternatively, we can
assign a score to
each sentence and take the top-k (or top-p) scores. It is very
reliable, as the information
given is sure to be from the original text, but it can be limited in fluency.

Abstractive summarization can use sequence-to-sequence models (such
as T5) to learn a mapping
between an input sequence and an output sequence. This can be trained
on pairs of
longer and shorter texts. Alternatively, we can use instruction-tuned
LLMs where we
provide a few examples for in-context learning. It provides a more
natural and fluent
result but there is a risk of hallucinations.

\subsection{Healthcare TM}

There is a large amount of data in healthcare which brings advantages
and challenges
for text mining tasks. Examples are demographic data, discharge
letters, imaging, diagnosis codes,
lab results or patient experience questionnaires. Aging population,
more chronic diseases and
innovation are drivers for applying text mining in healthcare.

\subsection{AutoScriber}

The idea behind autoscriber is to automate (part of) the tasks related
to medical annotation and documentation tasks from patient consultations.
It works by recording clinical conversations, generating a transcript from 
the conversation and feeding this text data into NLP tasks for entity recognition,
classification and summarization.

\subsubsection{Summarization}

As this research was done in 2021, the idea was to start with the automatic
transcript, preprocess it to remove stop words among others, and use tf-idf 
to extract keywords. These keywords are then filtered to extract relevant
keywords. Next, neural networks were applied (CNN-LSTM), and needed a lot 
more annotation.

One of the challenges is that the datasets are incomplete, they are collected
for a specific reason by a specific doctor, making it subjective and biased
data. Especially as the conversation is guided by the doctor, the contents will
change based on the suspicions and ideas from the doctor. For example, a cardiologist
will ask very different questions than a psychiatrist.
Another challenge was the correctness of the data, typos, abbreviations (that are ambiguous)
and missing metadata.

Finally, for summarization LLMs were employed as it is particularly necessary to
use fluent language. Extractive summarization will only contain parts of the conversation
and as there are multiple persons speaking this might give difficult to understand sentences
and results. Advantages are that no annotated data is necessary, and only examples are sufficient.
Moreover, low quality data is not a large issue here. However, it is difficult to evaluate the
outcomes of the LLMs and there is a risk of hallucinations.


